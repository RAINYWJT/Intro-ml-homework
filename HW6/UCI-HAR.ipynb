{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center\">机器学习导论习题六</h1>\n",
    "\n",
    "<h2 style=\"text-align: center;\">221300079, 王俊童, <a href=\"mailto:221300079@smail.nju.edu.cn\">221300079@smail.nju.edu.cn</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于记录每个单元格的运行时间\n",
    "\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入第三方库\n",
    "\n",
    "import os, re, glob, time, random, datetime\n",
    "import multiprocessing as mp\n",
    "\n",
    "GLOBAL_START_TIME = time.time()\n",
    "\n",
    "# # !pip install ipywidgets widgetsnbextension pandas-profiling\n",
    "# from tqdm.notebook import trange, tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "\n",
    "import mindspore\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 128\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固定随机数种子\n",
    "\n",
    "GLOBAL_SEED = 0\n",
    "\n",
    "def fix_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if 'mindspore' in globals():\n",
    "        mindspore.set_seed(seed)\n",
    "\n",
    "fix_seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 [20pts] 处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) [5pts] 加载数据\n",
    "\n",
    "从 `./data/` 中加载数据, 特征数据加载为 `np.float64`, 标签数据加载为 `np.int64`. 注意, 原始数据的标签从 `1` 开始, 你需要转换成从 `0` 开始."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "import numpy as np\n",
    "\n",
    "train_x = np.loadtxt('./har-data/train_x.txt', dtype=np.float64)\n",
    "train_y = np.loadtxt('./har-data/train_y.txt', dtype=np.int64) - 1  \n",
    "test_x = np.loadtxt('./har-data/test_x.txt', dtype=np.float64)\n",
    "test_y = np.loadtxt('./har-data/test_y.txt', dtype=np.int64) - 1 \n",
    "\n",
    "print(f'train_x.dtype={train_x.dtype}; train_x.shape={train_x.shape}; train_y.dtype={train_y.dtype}; train_y.shape={train_y.shape};')\n",
    "print(f'test_x.dtype={test_x.dtype}; test_x.shape={test_x.shape}; test_y.dtype={test_y.dtype}; test_y.shape={test_y.shape};')\n",
    "\n",
    "assert all((train_x.shape == (7352, 561), train_y.shape == (7352,), test_x.shape == (2947, 561), test_y.shape == (2947,)))\n",
    "assert all((train_x.dtype == np.float64, train_y.dtype == np.int64, test_x.dtype == np.float64, test_y.dtype == np.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) [5pts] 检查数据\n",
    "\n",
    "分析并回答如下问题:\n",
    "\n",
    "- 数据中是否存在缺失值?\n",
    "\n",
    "输出如下：\n",
    "\n",
    "Missing values in train_x: 0\n",
    "\n",
    "Missing values in train_y: 0\n",
    "\n",
    "Missing values in test_x: 0\n",
    "\n",
    "Missing values in test_y: 0\n",
    "\n",
    "输出全是0，说明没有缺失值。\n",
    "\n",
    "- 数据是否存在类别不平衡的问题?\n",
    "\n",
    "输出如下：\n",
    "\n",
    "Class counts in train_y: [1226 1073  986 1286 1374 1407]\n",
    "\n",
    "Class counts in test_y: [496 471 420 491 532 537]\n",
    "\n",
    "可以看到，test的有些类别完全的少于train类别，说明存在类别不平衡问题。\n",
    "\n",
    "- 数据属性取值是否需要归一化?\n",
    "\n",
    "输出如下：我就不在这里放出来了，太长了。助教老师可以自行的去打印出来看看，根据我在ide里面查看的结果是不大需要归一化的，因为好像均值和方差来说影响不大，就没必要归一化了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印训练数据缺失值的统计结果\n",
    "missing_train_x = np.isnan(train_x).sum()\n",
    "missing_train_y = np.isnan(train_y).sum()\n",
    "missing_test_x = np.isnan(test_x).sum()\n",
    "missing_test_y = np.isnan(test_y).sum()\n",
    "\n",
    "print(f'Missing values in train_x: {missing_train_x}')\n",
    "print(f'Missing values in train_y: {missing_train_y}')\n",
    "print(f'Missing values in test_x: {missing_test_x}')\n",
    "print(f'Missing values in test_y: {missing_test_y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印训练数据类别样例数量的统计结果\n",
    "\n",
    "# 提示: np.bincount\n",
    "class_counts_train = np.bincount(train_y)\n",
    "class_counts_test = np.bincount(test_y)\n",
    "\n",
    "print(f'Class counts in train_y: {class_counts_train}')\n",
    "print(f'Class counts in test_y: {class_counts_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印训练数据属性取值的统计结果\n",
    "train_x_min = np.min(train_x, axis=0)\n",
    "train_x_max = np.max(train_x, axis=0)\n",
    "train_x_mean = np.mean(train_x, axis=0)\n",
    "train_x_std = np.std(train_x, axis=0)\n",
    "\n",
    "print(f'Min values in train_x: {train_x_min}')\n",
    "print(f'Max values in train_x: {train_x_max}')\n",
    "print(f'Mean values in train_x: {train_x_mean}')\n",
    "print(f'Std values in train_x: {train_x_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) [5pts] 可视化属性分布\n",
    "\n",
    "属性取值归一化之后, 选择方差最大的特征, 绘制小提琴图, 可视化对比各个类别的样本在该属性上取值分布.\n",
    "\n",
    "绘图参考下图:\n",
    "\n",
    "<div><img src=\"./plot/violinplot.png\" width=\"512\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我的实验结果如下图所示：\n",
    "<div><img src=\"vl.png\" width=\"512\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 挑选方差最大的属性\n",
    "\n",
    "train_x = np.loadtxt('./har-data/train_x.txt', dtype=np.float64)\n",
    "train_y = np.loadtxt('./har-data/train_y.txt', dtype=np.int64) - 1  \n",
    "test_x = np.loadtxt('./har-data/test_x.txt', dtype=np.float64)\n",
    "test_y = np.loadtxt('./har-data/test_y.txt', dtype=np.int64) - 1 \n",
    "# 提示: np.argmax\n",
    "scaler = MinMaxScaler()\n",
    "train_x_scaled = scaler.fit_transform(train_x)\n",
    "\n",
    "# 选择方差最大的属性\n",
    "variances = np.var(train_x_scaled, axis=0)\n",
    "max_variance_index = np.argmax(variances)\n",
    "max_variance_feature = train_x_scaled[:, max_variance_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制小提琴图可视化各个类别在该属性上取值分布\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x=train_y, y=max_variance_feature)\n",
    "plt.title(f'Violin Plot of Feature with Maximum Variance (Index: {max_variance_index})')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Feature Value')\n",
    "plt.xticks(ticks=[0, 1, 2, 3, 4, 5], labels=['Walking', 'Upstairs', 'Downstairs', 'Sitting', 'Standing', 'Laying'])\n",
    "plt.savefig('violin.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) [5pts] 可视化属性相关性\n",
    "\n",
    "绘制热力图, 可视化前 51 个属性两两之间的 Pearson 相关系数.\n",
    "\n",
    "绘图参考下图:\n",
    "\n",
    "<div><img src=\"./plot/heatmap.png\" width=\"512\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是我画的图：\n",
    "\n",
    "<div><img src=\"51-sns.png\" width=\"512\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制热力图可视化前51个属性两两之间的Pearson相关系数\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 之前的处理同上面\n",
    "# 加载数据\n",
    "train_x = np.loadtxt('./har-data/train_x.txt', dtype=np.float64)\n",
    "train_y = np.loadtxt('./har-data/train_y.txt', dtype=np.int64) - 1  \n",
    "test_x = np.loadtxt('./har-data/test_x.txt', dtype=np.float64)\n",
    "test_y = np.loadtxt('./har-data/test_y.txt', dtype=np.int64) - 1 \n",
    "\n",
    "# 提示: sns.heatmap\n",
    "scaler = MinMaxScaler()\n",
    "train_x_scaled = scaler.fit_transform(train_x)\n",
    "selected_features = train_x_scaled[:, :51]\n",
    "correlation_matrix = np.corrcoef(selected_features, rowvar=False)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Heatmap of Pearson Correlation Coefficients for the First 51 Features')\n",
    "plt.savefig('51-sns.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 [15pts+附加5pts] 分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) [5pts] 调用 `sklearn` 实现基线模型\n",
    "\n",
    "固定超参数, 汇报如下基线模型的运行时间和准确率: $k$ 近邻, 高斯核支持向量机 (高斯核又称径向基核), 随机森林.\n",
    "\n",
    "> - $k$ 近邻: elapsed=????s; accuracy=????%; \n",
    "> - 高斯核支持向量机: elapsed=????s; accuracy=????%;\n",
    "> - 随机森林: elapsed=????s; accuracy=????%;\n",
    "\n",
    "答案如下：\n",
    "> - k 近邻: elapsed=0.2300s; accuracy=90.16%\n",
    "> - 高斯核支持向量机: elapsed=2.4248s; accuracy=95.05%\n",
    "> - 随机森林: elapsed=10.6345s; accuracy=92.57%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# 注意固定随机数种子确保实验可复现\n",
    "np.random.seed(42)\n",
    "train_x = np.loadtxt('./har-data/train_x.txt', dtype=np.float64)\n",
    "train_y = np.loadtxt('./har-data/train_y.txt', dtype=np.int64) - 1  \n",
    "test_x = np.loadtxt('./har-data/test_x.txt', dtype=np.float64)\n",
    "test_y = np.loadtxt('./har-data/test_y.txt', dtype=np.int64) - 1 \n",
    "# k 近邻\n",
    "start_time = time.time()\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(train_x, train_y)\n",
    "y_pred_knn = knn.predict(test_x)\n",
    "elapsed_time_knn = time.time() - start_time\n",
    "accuracy_knn = accuracy_score(test_y, y_pred_knn) * 100\n",
    "print(f\"k 近邻: elapsed={elapsed_time_knn:.4f}s; accuracy={accuracy_knn:.2f}%\")\n",
    "\n",
    "# 高斯核支持向量机\n",
    "start_time = time.time()\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm.fit(train_x, train_y)\n",
    "y_pred_svm = svm.predict(test_x)\n",
    "elapsed_time_svm = time.time() - start_time\n",
    "accuracy_svm = accuracy_score(test_y, y_pred_svm) * 100\n",
    "print(f\"高斯核支持向量机: elapsed={elapsed_time_svm:.4f}s; accuracy={accuracy_svm:.2f}%\")\n",
    "\n",
    "# 随机森林\n",
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(train_x, train_y)\n",
    "y_pred_rf = rf.predict(test_x)\n",
    "elapsed_time_rf = time.time() - start_time\n",
    "accuracy_rf = accuracy_score(test_y, y_pred_rf) * 100\n",
    "print(f\"随机森林: elapsed={elapsed_time_rf:.4f}s; accuracy={accuracy_rf:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) [5pts] 调用 `xgboost` 实现 Boosting 模型\n",
    "\n",
    "固定超参数, 汇报 `xgboost` 的运行时间和准确率.\n",
    "\n",
    "> - `xgboost`: elapsed=????s; accuracy=????%;\n",
    "\n",
    "答案如下：\n",
    "\n",
    "> - xgboost: elapsed=8.4554s; accuracy=93.93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "train_x = np.loadtxt('./har-data/train_x.txt', dtype=np.float64)\n",
    "train_y = np.loadtxt('./har-data/train_y.txt', dtype=np.int64) - 1  \n",
    "test_x = np.loadtxt('./har-data/test_x.txt', dtype=np.float64)\n",
    "test_y = np.loadtxt('./har-data/test_y.txt', dtype=np.int64) - 1 \n",
    "# 注意固定随机数种子确保实验可复现\n",
    "np.random.seed(42)\n",
    "\n",
    "# xgboost \n",
    "start_time = time.time()\n",
    "xgboost = XGBClassifier()\n",
    "xgboost.fit(train_x, train_y)\n",
    "y_pred_xgboost = xgboost.predict(test_x)\n",
    "elapsed_time_xgboost = time.time() - start_time\n",
    "accuracy_knn = accuracy_score(test_y, y_pred_xgboost) * 100\n",
    "print(f\"xgboost: elapsed={elapsed_time_xgboost:.4f}s; accuracy={accuracy_knn:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) [5pts] 基于 `torch` 训练神经网络模型\n",
    "\n",
    "每遍历一轮训练数据, 就在 `./ckpt/` 中保存当前模型权重, 固定超参数, 绘制神经网络在训练集和测试集上的准确率随训练轮数变化的折线图.\n",
    "\n",
    "绘图参考下图:\n",
    "\n",
    "<div><img src=\"./plot/line.png\" width=\"512\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我的实现如下：此处随机种子取的数据是0，试了一下，如果取到大一点比如42或者21之类的，起点会低一些，但是后面也差不多。\n",
    "\n",
    "<div><img src=\"torch.png\" width=\"512\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: list, output_dim: int):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.hidden_layers = [\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim_in, dim_out),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            for dim_in, dim_out in zip(hidden_dims[:-1], hidden_dims[+1:])\n",
    "        ]\n",
    "        self.hidden_layers = nn.Sequential(*self.hidden_layers)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[-1], output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = hidden_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "class MLPClassifier(object):\n",
    "    def __init__(self, hidden_dims: list = [128, 32], batch_size: int = 128, learning_rate: float = 1e-2, num_epochs: int = 16, ckpt_dir: str = None):\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "\n",
    "    def define_mlp(self, x: np.ndarray, y: np.ndarray):\n",
    "        _, self.input_dim = x.shape\n",
    "        self.output_dim = y.max() + 1\n",
    "        self.mlp = MLP(input_dim=self.input_dim, hidden_dims=self.hidden_dims, output_dim=self.output_dim)\n",
    "    \n",
    "    def train_mlp(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.mlp.train()\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(y).long()\n",
    "        dataset = TensorDataset(x, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.mlp.parameters(), lr=self.learning_rate)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in dataloader:\n",
    "                # raise NotImplementedError()\n",
    "                # TODO: 阅读官方文档示例, 完成梯度下降的代码, 注意把当前批次的损失加到 running_loss 上.\n",
    "                # https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.mlp(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()  \n",
    "                optimizer.step() \n",
    "                running_loss += loss.item()  \n",
    "            print(f'\\033[1m[{epoch+1:3d}/{self.num_epochs:3d}]\\033[0m running_loss={running_loss:8.4f};')\n",
    "            if self.ckpt_dir:\n",
    "                torch.save(self.mlp.state_dict(), os.path.join(self.ckpt_dir, f'{epoch:03d}.pt'))\n",
    "\n",
    "    def load(self, x: np.ndarray, y: np.ndarray, ckpt_path: str):\n",
    "        self.define_mlp(x=x, y=y)\n",
    "        assert os.path.exists(ckpt_path)\n",
    "        self.mlp.load_state_dict(torch.load(ckpt_path))\n",
    "    \n",
    "    def fit(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.define_mlp(x=x, y=y)\n",
    "        self.train_mlp(x=x, y=y)\n",
    "    \n",
    "    def predict(self, x: np.ndarray):\n",
    "        self.mlp.eval()\n",
    "        x = torch.from_numpy(x).float()\n",
    "        dataset = TensorDataset(x)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, in dataloader:\n",
    "                outputs = self.mlp(inputs)\n",
    "                labels = outputs.argmax(dim=1)\n",
    "                y_pred.append(labels)\n",
    "        y_pred = torch.cat(y_pred, dim=0)\n",
    "        return y_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "train_x = np.loadtxt('./har-data/train_x.txt', dtype=np.float64)\n",
    "train_y = np.loadtxt('./har-data/train_y.txt', dtype=np.int64) - 1  \n",
    "test_x = np.loadtxt('./har-data/test_x.txt', dtype=np.float64)\n",
    "test_y = np.loadtxt('./har-data/test_y.txt', dtype=np.int64) - 1 \n",
    "\n",
    "# 固定随机数种子\n",
    "\n",
    "GLOBAL_SEED = 0\n",
    "\n",
    "def fix_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "fix_seed(GLOBAL_SEED)\n",
    "classifier = MLPClassifier(ckpt_dir='./ckpt/')\n",
    "\n",
    "# 不知道为什么fix seed 用不了，这里直接用的。\n",
    "_start_time = time.time()\n",
    "classifier.fit(train_x, train_y)\n",
    "test_y_hat = classifier.predict(test_x)\n",
    "accuracy = accuracy_score(test_y, test_y_hat)\n",
    "_end_time = time.time()\n",
    "_elapsed_time = _end_time - _start_time\n",
    "print(f'\\033[1m[{classifier.__class__.__name__}]\\033[0m elapsed={_elapsed_time:.2f}s; accuracy={accuracy*100:.2f}%;')\n",
    "\n",
    "del classifier, test_y_hat, accuracy\n",
    "del _start_time, _end_time, _elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 绘制折线图可视化神经网络在训练集和测试集上的准确率随训练轮数变化\n",
    "classifier = MLPClassifier(ckpt_dir=None)\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "for epoch in range(16):\n",
    "    classifier.load(train_x, train_y, os.path.join('./ckpt/', f'{epoch:03d}.pt'))\n",
    "    train_y_hat = classifier.predict(train_x)\n",
    "    train_acc = accuracy_score(train_y, train_y_hat)\n",
    "    train_accs.append(train_acc)\n",
    "    test_y_hat = classifier.predict(test_x)\n",
    "    test_acc = accuracy_score(test_y, test_y_hat)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    \n",
    "print(f'train_accs = {train_accs};')\n",
    "print(f'test_accs = {test_accs};')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(0, 16), train_accs, marker='o', linestyle='-', color='r', label='Train Accuracy')\n",
    "plt.plot(range(0, 16), test_accs, marker='o', linestyle='-', color='b', label='Test Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.savefig('torch-ckpt.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) [附加5pts] 基于 `mindspore` 训练神经网络模型\n",
    "\n",
    "使用国产化软件复现 (3) 的结果, 并比较二者在效率等方面的差异.\n",
    "\n",
    "我的实现如下：此处随机种子取的数据是0，额事实上，可以看一下他的具体数据：\n",
    "<div><img src=\"mindspore2.png\" width=\"512\"/></div>\n",
    "\n",
    "综合对比一下torch，其实差不多，准确率上没啥太大的差别，但是运行速度比torch快一点，这个是很好的，准确率稍高一点点吧。\n",
    "\n",
    "<div><img src=\"mindspore.png\" width=\"512\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "\n",
    "class MLP(nn.Cell):\n",
    "    def __init__(self, input_dim: int, hidden_dims: list, output_dim: int):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_layer = nn.SequentialCell(\n",
    "            nn.Dense(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.hidden_layers = [\n",
    "            nn.SequentialCell(\n",
    "                nn.Dense(dim_in, dim_out),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            for dim_in, dim_out in zip(hidden_dims[:-1], hidden_dims[+1:])\n",
    "        ]\n",
    "        self.hidden_layers = nn.SequentialCell(*self.hidden_layers)\n",
    "        self.output_layer = nn.SequentialCell(\n",
    "            nn.Dense(hidden_dims[-1], output_dim),\n",
    "        )\n",
    "    def construct(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = hidden_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "class MLPClassifier(object):\n",
    "    def __init__(self, hidden_dims: list = [128, 32], batch_size: int = 128, learning_rate: float = 1e-2, num_epochs: int = 16, ckpt_dir: str = None):\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "    def define_mlp(self, x: np.ndarray, y: np.ndarray):\n",
    "        _, self.input_dim = x.shape\n",
    "        self.output_dim = y.max() + 1\n",
    "        self.input_dim = int(self.input_dim)\n",
    "        self.output_dim = int(self.output_dim)\n",
    "        self.mlp = MLP(input_dim=self.input_dim, hidden_dims=self.hidden_dims, output_dim=self.output_dim)\n",
    "    def train_mlp(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.mlp.set_train(True)\n",
    "        x = mindspore.Tensor.from_numpy(x).float()\n",
    "        y = mindspore.Tensor.from_numpy(y).int()\n",
    "        dataset = mindspore.dataset.GeneratorDataset([(xi, yi) for xi, yi in zip(x, y)], column_names=['inputs', 'labels'], shuffle=True).batch(batch_size=self.batch_size)\n",
    "        dataloader = dataset.create_tuple_iterator()\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = nn.Adam(self.mlp.trainable_params(), learning_rate=self.learning_rate)\n",
    "        def forward_fn(inputs, labels):\n",
    "            logits = self.mlp(inputs)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            return loss, logits\n",
    "        grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            loss = 0.0\n",
    "            for inputs, labels in dataloader:\n",
    "                # raise NotImplementedError()\n",
    "                # TODO: 阅读官方文档示例, 完成梯度下降的代码, 注意把当前批次的损失加到 running_loss 上.\n",
    "                # https://www.mindspore.cn/tutorials/zh-CN/r2.3.0rc2/beginner/train.html#%E8%AE%AD%E7%BB%83%E4%B8%8E%E8%AF%84%E4%BC%B0\n",
    "                (loss_1, _), grads = grad_fn(inputs, labels)\n",
    "                loss += loss_1.asnumpy()\n",
    "                optimizer(grads)\n",
    "\n",
    "            print(f'\\033[1m[{epoch+1:3d}/{self.num_epochs:3d}]\\033[0m loss={loss:8.4f};')\n",
    "            if self.ckpt_dir:\n",
    "                mindspore.save_checkpoint(self.mlp, os.path.join(self.ckpt_dir, f'{epoch:03d}.ckpt'))\n",
    "    def load(self, x: np.ndarray, y: np.ndarray, ckpt_path: str):\n",
    "        self.define_mlp(x=x, y=y)\n",
    "        assert os.path.exists(ckpt_path)\n",
    "        _, _ = mindspore.load_param_into_net(self.mlp, mindspore.load_checkpoint(ckpt_path))\n",
    "    def fit(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.define_mlp(x=x, y=y)\n",
    "        self.train_mlp(x=x, y=y)\n",
    "    def predict(self, x: np.ndarray):\n",
    "        self.mlp.set_train(False)\n",
    "        x = mindspore.Tensor.from_numpy(x).float()\n",
    "        dataset = mindspore.dataset.GeneratorDataset([xi for xi in x], column_names=['inputs'], shuffle=False).batch(batch_size=self.batch_size)\n",
    "        dataloader = dataset.create_tuple_iterator()\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, in dataloader:\n",
    "                outputs = self.mlp(inputs)\n",
    "                labels = outputs.argmax(axis=1)\n",
    "                y_pred.append(labels)\n",
    "        y_pred = ops.Concat(axis=0)(y_pred)\n",
    "        return y_pred.asnumpy()\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "train_x = np.loadtxt('./har-data/train_x.txt', dtype=np.float64)\n",
    "train_y = np.loadtxt('./har-data/train_y.txt', dtype=np.int64) - 1  \n",
    "test_x = np.loadtxt('./har-data/test_x.txt', dtype=np.float64)\n",
    "test_y = np.loadtxt('./har-data/test_y.txt', dtype=np.int64) - 1 \n",
    "\n",
    "# 设置随机种子\n",
    "GLOBAL_SEED = 42\n",
    "torch.manual_seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "\n",
    "classifier = MLPClassifier(ckpt_dir='./ckpt/')\n",
    "\n",
    "_start_time = time.time()\n",
    "classifier.fit(train_x, train_y)\n",
    "test_y_hat = classifier.predict(test_x)\n",
    "accuracy = accuracy_score(test_y, test_y_hat)\n",
    "_end_time = time.time()\n",
    "_elapsed_time = _end_time - _start_time\n",
    "print(f'\\033[1m[{classifier.__class__.__name__}]\\033[0m elapsed={_elapsed_time:.2f}s; accuracy={accuracy*100:.2f}%;')\n",
    "\n",
    "del classifier, test_y_hat, accuracy\n",
    "del _start_time, _end_time, _elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制折线图可视化神经网络在训练集和测试集上的准确率随训练轮数变化\n",
    "\n",
    "\n",
    "classifier = MLPClassifier(ckpt_dir=None)\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "for epoch in range(16):\n",
    "    classifier.load(train_x, train_y, os.path.join('./ckpt/', f'{epoch:03d}.ckpt'))\n",
    "    train_y_hat = classifier.predict(train_x)\n",
    "    train_acc = accuracy_score(train_y, train_y_hat)\n",
    "    train_accs.append(train_acc)\n",
    "    test_y_hat = classifier.predict(test_x)\n",
    "    test_acc = accuracy_score(test_y, test_y_hat)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "print(f'train_accs = {train_accs};')\n",
    "print(f'test_accs = {test_accs};')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(0, 16), train_accs, marker='o', linestyle='-', color='r', label='Train Accuracy')\n",
    "plt.plot(range(0, 16), test_accs, marker='o', linestyle='-', color='b', label='Test Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.savefig('mindspore.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 [15pts] 参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) [5pts] 5 折交叉验证\n",
    "\n",
    "调用 `sklearn` 实现, 为 $k$ 近邻选择最优的邻居数量 $k$, 汇报在训练集上选出的 $k$ 及其 5 折交叉验证准确率, $k \\in \\{1, \\cdots, 16\\}$. \n",
    "\n",
    "> - $k$=1, accuracy=????%;\n",
    "> - $k$=2, accuracy=????%;\n",
    "> - $k$=3, accuracy=????%;\n",
    "> - $k$=4, accuracy=????%;\n",
    "> - $k$=5, accuracy=????%;\n",
    "> - $k$=6, accuracy=????%;\n",
    "> - $k$=7, accuracy=????%;\n",
    "> - $k$=8, accuracy=????%;\n",
    "> - $k$=9, accuracy=????%;\n",
    "> - $k$=10, accuracy=????%;\n",
    "> - $k$=11, accuracy=????%;\n",
    "> - $k$=12, accuracy=????%;\n",
    "> - $k$=13, accuracy=????%;\n",
    "> - $k$=14, accuracy=????%;\n",
    "> - $k$=15, accuracy=????%;\n",
    "> - $k$=16, accuracy=????%;\n",
    "\n",
    "我的答案如下\n",
    "> - k=1, Accuracy: 0.8792\n",
    "> - k=2, Accuracy: 0.8678\n",
    "> - k=3, Accuracy: 0.8919\n",
    "> - k=4, Accuracy: 0.8902\n",
    "> - k=5, Accuracy: 0.8968\n",
    "> - k=6, Accuracy: 0.8953\n",
    "> - k=7, Accuracy: 0.8996\n",
    "> - k=8, Accuracy: 0.8984\n",
    "> - k=9, Accuracy: 0.8983\n",
    "> - k=10, Accuracy: 0.8970\n",
    "> - k=11, Accuracy: 0.8985\n",
    "> - k=12, Accuracy: 0.8983\n",
    "> - k=13, Accuracy: 0.8996\n",
    "> - k=14, Accuracy: 0.9008\n",
    "> - k=15, Accuracy: 0.9018\n",
    "> - k=16, Accuracy: 0.9019\n",
    "> - Best k: 16, Best Accuracy: 0.9019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from evaluate import evaluate_5_fold_cv\n",
    "import numpy as np\n",
    "\n",
    "# 加载数据\n",
    "train_x = np.loadtxt('./har-data/train_x.txt', dtype=np.float64)\n",
    "train_y = np.loadtxt('./har-data/train_y.txt', dtype=np.int64) - 1  \n",
    "test_x = np.loadtxt('./har-data/test_x.txt', dtype=np.float64)\n",
    "test_y = np.loadtxt('./har-data/test_y.txt', dtype=np.int64) - 1 \n",
    "\n",
    "k_values = range(1, 17)\n",
    "\n",
    "best_k = None\n",
    "best_accuracy = 0.0\n",
    "for k in k_values:\n",
    "    accuracy = evaluate_5_fold_cv(KNeighborsClassifier, [k], {}, train_x, train_y)\n",
    "    print(f\"k={k}, Accuracy: {accuracy:.4f}\")\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_k = k\n",
    "\n",
    "print(f\"Best k: {best_k}, Best Accuracy: {best_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) [5pts] 多进程并行加速\n",
    "\n",
    "为高斯核支持向量机选择最优的正则化系数 $C$, 汇报在训练集上选出的 $C$ 及其5折交叉验证准确率, 同时汇报使用多进程并行加速后的总用时, $C \\in \\{0.01, 0.1, 1.0, 10.0, 100.0\\}$.\n",
    "\n",
    "> - $C$=0.01, accuracy=????%\n",
    "> - $C$=0.1, accuracy=????%\n",
    "> - $C$=1.0, accuracy=????%\n",
    "> - $C$=10.0, accuracy=????%\n",
    "> - $C$=100.0, accuracy=????%\n",
    "> - 总共用时 ????s.\n",
    "\n",
    "我的答案如下：\n",
    "> - C=0.01, Mean Accuracy: 0.6193\n",
    "> - C=0.1, Mean Accuracy: 0.8904\n",
    "> - C=1.0, Mean Accuracy: 0.9313\n",
    "> - C=10.0, Mean Accuracy: 0.9426\n",
    "> - C=100.0, Mean Accuracy: 0.9453\n",
    "> - Best C: 100.0, Best Accuracy: 0.9453\n",
    "> - Total time: 42.14 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from evaluate import distributed_evaluate_classifier\n",
    "import numpy as np\n",
    "import time\n",
    "# 提示: 推荐使用 `joblib`, 其接口比 `multiprocessing` 更加简洁.\n",
    "# 注意: 如果你使用的操作系统是 Windows, 那么进程的入口函数必须从包文件导入.\n",
    "# 进程的入口函数已经实现, 即 distributed_evaluate_classifier, 你需要为其填充参数, 填充示例如下:\n",
    "# packed = [task_id, SVC, [], {'C': c, 'kernel': 'rbf'}, fit_x, fit_y, predict_x, predict_y]\n",
    "# 每个进程将执行如下代码:\n",
    "# distributed_evaluate_classifier(packed)\n",
    "# 其中, task_id 用来记录当前任务身份, 对于此处代码而言 task_id 应当包括 C 的值和交叉验证中的折数.\n",
    "# 其中, fit_x 和 fit_y 是这一折交叉验证的训练数据, predict_x 和 predict_y 是这一折交叉验证的测试数据.\n",
    "delayed_entrance = delayed(distributed_evaluate_classifier)\n",
    "\n",
    "# 加载数据\n",
    "train_x = np.loadtxt('./har-data/train_x.txt', dtype=np.float64)\n",
    "train_y = np.loadtxt('./har-data/train_y.txt', dtype=np.int64) - 1  \n",
    "test_x = np.loadtxt('./har-data/test_x.txt', dtype=np.float64)\n",
    "test_y = np.loadtxt('./har-data/test_y.txt', dtype=np.int64) - 1 \n",
    "\n",
    "C_values = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "tasks = []\n",
    "\n",
    "for c in C_values:\n",
    "    for fold, (fit_index, predict_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        fit_x = train_x[fit_index]\n",
    "        fit_y = train_y[fit_index]\n",
    "        predict_x = train_x[predict_index]\n",
    "        predict_y = train_y[predict_index]\n",
    "        task_id = (c, fold)\n",
    "        packed = [task_id, SVC, [], {'C': c, 'kernel': 'rbf'}, fit_x, fit_y, predict_x, predict_y]\n",
    "        tasks.append(packed)\n",
    "\n",
    "# 并行执行任务\n",
    "start_time = time.time()\n",
    "results = Parallel(n_jobs=-1)(delayed(distributed_evaluate_classifier)(task) for task in tasks)\n",
    "end_time = time.time()\n",
    "\n",
    "results_dict = {}\n",
    "for task_id, accuracy in results:\n",
    "    c, fold = task_id\n",
    "    if c not in results_dict:\n",
    "        results_dict[c] = []\n",
    "    results_dict[c].append(accuracy)\n",
    "\n",
    "# 计算平均准确率\n",
    "best_c = None\n",
    "best_accuracy = 0.0\n",
    "for c, accuracies in results_dict.items():\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    print(f\"C={c}, Mean Accuracy: {mean_accuracy:.4f}\")\n",
    "    if mean_accuracy > best_accuracy:\n",
    "        best_accuracy = mean_accuracy\n",
    "        best_c = c\n",
    "\n",
    "print(f\"Best C: {best_c}, Best Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"Total time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) [5pts] 搜索超参数\n",
    "\n",
    "使用 `optuna` 搜索 `xgboost` 的超参数, 汇报在训练集上选出的超参数及其 5 折交叉验证准确率 (更换随机数种子不低于 93.0%).\n",
    "\n",
    "> - 关键超参数: n_estimators=????; max_depth=????; max_leaves=????; eta=????; 如果你搜索了其他超参数也一并填在这里\n",
    "> - 5 折交叉验证准确率: ????%\n",
    "\n",
    "定义了一些有趣的超参数和范围，训了5轮，得到的如下：\n",
    "> - [I 2024-07-07 11:56:33,781] A new study created in memory with name: no-name-1c02b708-0948-4408-a560-e54ef37b1359\n",
    "> - [I 2024-07-07 11:56:54,754] Trial 0 finished with value: 0.9223358629651723 and parameters: {'n_estimators': 156, 'max_depth': 6, 'max_leaves': 3, 'learning_rate': 0.010773811090595426, 'gamma': 0.17734959122195582, 'min_child_weight': 2.7383357311250887}. Best is trial 0 with value: 0.9223358629651723.\n",
    "> - [I 2024-07-07 11:57:57,104] Trial 1 finished with value: 0.9892547528868789 and parameters: {'n_estimators': 530, 'max_depth': 10, 'max_leaves': 34, 'learning_rate': 0.09945815745816504, 'gamma': 0.31178419791902634, 'min_child_weight': 7.068064099823479}. Best is trial 1 with value: 0.9892547528868789.\n",
    "> - [I 2024-07-07 11:59:31,856] Trial 2 finished with value: 0.991158682371657 and parameters: {'n_estimators': 865, 'max_depth': 9, 'max_leaves': 81, 'learning_rate': 0.09838141992078817, 'gamma': 0.07550182350951207, 'min_child_weight': 5.9363860454124895}. Best is trial 2 with value: 0.991158682371657.\n",
    "> - [I 2024-07-07 12:07:39,101] Trial 3 finished with value: 0.9872144915070038 and parameters: {'n_estimators': 973, 'max_depth': 10, 'max_leaves': 81, 'learning_rate': 0.021941288643226562, 'gamma': 0.9482209071462405, 'min_child_weight': 9.776082097219446}. Best is trial 2 with value: 0.991158682371657.\n",
    "> - [I 2024-07-07 12:09:04,791] Trial 4 finished with value: 0.9897986006095163 and parameters: {'n_estimators': 628, 'max_depth': 3, 'max_leaves': 94, 'learning_rate': 0.03976597650312424, 'gamma': 0.6595717254443955, 'min_child_weight': 5.734266288352435}. Best is trial 2 with value: 0.991158682371657.\n",
    "\n",
    "Best trial:\n",
    "  > - Value: 0.9912\n",
    "  > - Params: \n",
    "  > -   n_estimators: 865\n",
    "  > -   max_depth: 9\n",
    "  > -   max_leaves: 81\n",
    "  > -   learning_rate: 0.09838141992078817\n",
    "  > -   gamma: 0.07550182350951207\n",
    "  > -   min_child_weight: 5.9363860454124895"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from evaluate import evaluate_5_fold_cv\n",
    "import optuna\n",
    "import numpy as np\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "# 加载数据\n",
    "train_x = np.loadtxt('./har-data/train_x.txt', dtype=np.float64)\n",
    "train_y = np.loadtxt('./har-data/train_y.txt', dtype=np.int64) - 1  \n",
    "test_x = np.loadtxt('./har-data/test_x.txt', dtype=np.float64)\n",
    "test_y = np.loadtxt('./har-data/test_y.txt', dtype=np.int64) - 1 \n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'max_leaves': trial.suggest_int('max_leaves', 0, 100),  \n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1, 10),\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=93)\n",
    "    accuracies = []\n",
    "    for train_idx, val_idx in kf.split(train_x, train_y):\n",
    "        train_fold_x, train_fold_y = train_x[train_idx], train_y[train_idx]\n",
    "        val_fold_x, val_fold_y = train_x[val_idx], train_y[val_idx]\n",
    "        model.fit(train_fold_x, train_fold_y)\n",
    "        val_pred = model.predict(val_fold_x)\n",
    "        accuracy = accuracy_score(val_fold_y, val_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "print(\"Best trial:\")\n",
    "print(\"  Value: {:.4f}\".format(study.best_value))\n",
    "print(\"  Params: \")\n",
    "for key, value in study.best_params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 [15pts] 集成模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) [5pts] 简单多数投票\n",
    "\n",
    "采用简单多数投票法集成之前题目调过参的分类器: $k$ 近邻, 高斯核支持向量机, `xgboost`.\n",
    "\n",
    "汇报测试集上准确率的提升.\n",
    "\n",
    "> 测试集上准确率的提升: .\n",
    "\n",
    "答案如下：\n",
    "> - KNN Accuracy: 0.9060\n",
    "> - SVM Accuracy: 0.9654\n",
    "> - XGBoost Accuracy: 0.9386\n",
    "> - Ensemble Accuracy: 0.9569\n",
    "> - Accuracy Improvement: -0.0085\n",
    "\n",
    "可以看到的是我们对于三个学习器进行简单投票集成之后，精度基本是跟svm差不多了，根据好而不同的要求，其实这个集成还是挺成功的，绝对是提升了的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 加载数据\n",
    "train_x = np.loadtxt('./har-data/train_x.txt', dtype=np.float64)\n",
    "train_y = np.loadtxt('./har-data/train_y.txt', dtype=np.int64) - 1  \n",
    "test_x = np.loadtxt('./har-data/test_x.txt', dtype=np.float64)\n",
    "test_y = np.loadtxt('./har-data/test_y.txt', dtype=np.int64) - 1\n",
    "\n",
    "# Best k: 16, Best Accuracy: 0.9019\n",
    "knn = KNeighborsClassifier(n_neighbors=16)\n",
    "# Best C: 100.0, Best Accuracy: 0.9453\n",
    "svm = SVC(C=100, kernel='rbf')\n",
    "# n_estimators': 865, 'max_depth': 9, 'max_leaves': 81, 'learning_rate': 0.09838141992078817, 'gamma': 0.07550182350951207, 'min_child_weight': 5.9363860454124895\n",
    "xgb = XGBClassifier(n_estimators=865, max_depth=9, learning_rate=0.09838141992078817, max_leaves=81, gamma=0.07550182350951207, min_child_weight=5.9363860454124895)\n",
    "\n",
    "knn.fit(train_x, train_y)\n",
    "svm.fit(train_x, train_y)\n",
    "xgb.fit(train_x, train_y)\n",
    "\n",
    "knn_pred = knn.predict(test_x)\n",
    "svm_pred = svm.predict(test_x)\n",
    "xgb_pred = xgb.predict(test_x)\n",
    "\n",
    "ensemble_pred = np.array([knn_pred, svm_pred, xgb_pred])\n",
    "ensemble_pred = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=ensemble_pred)\n",
    "\n",
    "knn_acc = accuracy_score(test_y, knn_pred)\n",
    "svm_acc = accuracy_score(test_y, svm_pred)\n",
    "xgb_acc = accuracy_score(test_y, xgb_pred)\n",
    "ensemble_acc = accuracy_score(test_y, ensemble_pred)\n",
    "\n",
    "print(f\"KNN Accuracy: {knn_acc:.4f}\")\n",
    "print(f\"SVM Accuracy: {svm_acc:.4f}\")\n",
    "print(f\"XGBoost Accuracy: {xgb_acc:.4f}\")\n",
    "print(f\"Ensemble Accuracy: {ensemble_acc:.4f}\")\n",
    "print(f\"Accuracy Improvement: {ensemble_acc - max(knn_acc, svm_acc, xgb_acc):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) [5pts] Stacking\n",
    "\n",
    "采用 Stacking 集成上一问中的分类器, 通过 5 折交叉验证训练 Stacking 模型. 汇报测试集上准确率的提升.\n",
    "\n",
    "> 测试集上准确率的提升: .\n",
    "\n",
    "我的答案如下：\n",
    "> - KNN Accuracy: 0.9060\n",
    "> - SVM Accuracy: 0.9654\n",
    "> - XGBoost Accuracy: 0.9386\n",
    "> - Stacking Accuracy: 0.9640\n",
    "> - Accuracy Improvement: -0.0014\n",
    "\n",
    "可以看出stacking方法进一步提升了准确率，集成是比较成功的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 加载数据\n",
    "train_x = np.loadtxt('./har-data/train_x.txt', dtype=np.float64)\n",
    "train_y = np.loadtxt('./har-data/train_y.txt', dtype=np.int64) - 1  \n",
    "test_x = np.loadtxt('./har-data/test_x.txt', dtype=np.float64)\n",
    "test_y = np.loadtxt('./har-data/test_y.txt', dtype=np.int64) - 1\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=16)\n",
    "svm = SVC(C=100, kernel='rbf', probability=True)\n",
    "xgb = XGBClassifier(n_estimators=865, max_depth=9, learning_rate=0.09838141992078817, max_leaves=81, gamma=0.07550182350951207, min_child_weight=5.9363860454124895)\n",
    "\n",
    "meta_classifier = LogisticRegression()\n",
    "\n",
    "stacking_classifier = StackingClassifier(\n",
    "    estimators=[('knn', knn), ('svm', svm), ('xgb', xgb)],\n",
    "    final_estimator=meta_classifier,\n",
    "    cv=StratifiedKFold(n_splits=5),\n",
    "    stack_method='predict_proba'\n",
    ")\n",
    "\n",
    "stacking_classifier.fit(train_x, train_y)\n",
    "stacking_pred = stacking_classifier.predict(test_x)\n",
    "\n",
    "knn.fit(train_x, train_y)\n",
    "svm.fit(train_x, train_y)\n",
    "xgb.fit(train_x, train_y)\n",
    "\n",
    "knn_pred = knn.predict(test_x)\n",
    "svm_pred = svm.predict(test_x)\n",
    "xgb_pred = xgb.predict(test_x)\n",
    "\n",
    "knn_acc = accuracy_score(test_y, knn_pred)\n",
    "svm_acc = accuracy_score(test_y, svm_pred)\n",
    "xgb_acc = accuracy_score(test_y, xgb_pred)\n",
    "stacking_acc = accuracy_score(test_y, stacking_pred)\n",
    "\n",
    "print(f\"KNN Accuracy: {knn_acc:.4f}\")\n",
    "print(f\"SVM Accuracy: {svm_acc:.4f}\")\n",
    "print(f\"XGBoost Accuracy: {xgb_acc:.4f}\")\n",
    "print(f\"Stacking Accuracy: {stacking_acc:.4f}\")\n",
    "print(f\"Accuracy Improvement: {stacking_acc - max(knn_acc, svm_acc, xgb_acc):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) [5pts] 探索其他集成方式\n",
    "\n",
    "以下方式任选其一: 把神经网络最后一个隐层的输出作为新的特征, 训练一个根据样本决定采用哪个模型的路由模型, 提出你自己的集成方式并给出清晰的说明. 汇报测试集上准确率的提升.\n",
    "\n",
    "> 测试集上准确率的提升: .\n",
    "\n",
    "我的答案如下：\n",
    "> - KNN Accuracy: 0.9060\n",
    "> - SVM Accuracy: 0.9654\n",
    "> - XGBoost Accuracy: 0.9386\n",
    "> - Custom Ensemble Accuracy: 0.9603\n",
    "> - Accuracy Improvement: -0.0051\n",
    "\n",
    "我采用了一种加权平均的集成方式，有的效果差一些，权重就应当小一些，而更准确的模型的权重应当更大，所以结果可以看出效果确实还是不错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 加载数据\n",
    "train_x = np.loadtxt('./har-data/train_x.txt', dtype=np.float64)\n",
    "train_y = np.loadtxt('./har-data/train_y.txt', dtype=np.int64) - 1  \n",
    "test_x = np.loadtxt('./har-data/test_x.txt', dtype=np.float64)\n",
    "test_y = np.loadtxt('./har-data/test_y.txt', dtype=np.int64) - 1\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=16)\n",
    "svm = SVC(C=100, kernel='rbf', probability=True)\n",
    "xgb = XGBClassifier(n_estimators=865, max_depth=9, learning_rate=0.09838141992078817, max_leaves=81, gamma=0.07550182350951207, min_child_weight=5.9363860454124895)\n",
    "\n",
    "knn.fit(train_x, train_y)\n",
    "svm.fit(train_x, train_y)\n",
    "xgb.fit(train_x, train_y)\n",
    "\n",
    "knn_proba = knn.predict_proba(test_x)\n",
    "svm_proba = svm.predict_proba(test_x)\n",
    "xgb_proba = xgb.predict_proba(test_x)\n",
    "\n",
    "knn_pred = knn.predict(test_x)\n",
    "svm_pred = svm.predict(test_x)\n",
    "xgb_pred = xgb.predict(test_x)\n",
    "\n",
    "knn_acc = accuracy_score(test_y, knn_pred)\n",
    "svm_acc = accuracy_score(test_y, svm_pred)\n",
    "xgb_acc = accuracy_score(test_y, xgb_pred)\n",
    "sum_acc = knn_acc + svm_acc + xgb_acc\n",
    "# 加权平均集成\n",
    "weights = [knn_acc/sum_acc, svm_acc/sum_acc, xgb_acc/sum_acc]\n",
    "ensemble_proba = np.average([knn_proba, svm_proba, xgb_proba], axis=0, weights=weights)\n",
    "ensemble_pred = np.argmax(ensemble_proba, axis=1)\n",
    "ensemble_acc = accuracy_score(test_y, ensemble_pred)\n",
    "\n",
    "print(f\"KNN Accuracy: {knn_acc:.4f}\")\n",
    "print(f\"SVM Accuracy: {svm_acc:.4f}\")\n",
    "print(f\"XGBoost Accuracy: {xgb_acc:.4f}\")\n",
    "print(f\"Custom Ensemble Accuracy: {ensemble_acc:.4f}\")\n",
    "print(f\"Accuracy Improvement: {ensemble_acc - max(knn_acc, svm_acc, xgb_acc):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 [附加5pts] 学件市场\n",
    "\n",
    "<h2>北冥坞的注册邮箱: <code>221300079@smail.nju.edu.cn</code></h2>\n",
    "\n",
    "<h2>上传的学件的 ID: <code>UCI-HAR_ML_HW6</code></h2>\n",
    "\n",
    "<h2>根据规约查搜学件的截图:</h2>\n",
    "<div><img src=\"BMW.png\" width=\"512\"/></div>\n",
    "\n",
    "<h2>你认为北冥坞还需要改进的地方: (例如, 代码报错日志不够详细, 上传/复用学件存在明显的冗余操作步骤, &hellip;)</h2>\n",
    "\n",
    "还是有一些问题的：\n",
    "> - 1.在进行本地检查的时候我一直找不到路径，结果发现多套了一层文件夹，这个简单情形道理上讲是应该能处理的，而且网站的手册也没有对用户的行为进行规范。\n",
    "> - 2.本地检查时anaconda有可能连接不顺畅导致检查失败。\n",
    "> - 3.应当给出目前大部分像numpy，sklearn的兼容版本的说明在网站上，而且看起来对于新版本兼容性不高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 致谢\n",
    "\n",
    "<h2>允许与其他同样未完成作业的同学讨论作业的内容, 但需在此注明并加以致谢; 如在作业过程中, 参考了互联网上的资料或大语言模型的生成结果, 且对完成作业有帮助的, 亦需注明并致谢.</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
